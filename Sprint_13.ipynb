{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 13 - TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous scratches, we:\n",
    "- Had to initialize the weights\n",
    "- Needed an epoch loop\n",
    "- Coded the Activation Functions\n",
    "- Decided the learning rate, sizes, number of nodes and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 0.0213, val_loss : 0.3141, acc : 1.000, val_acc : 0.875\n",
      "Epoch 1, loss : 0.0001, val_loss : 0.7273, acc : 1.000, val_acc : 0.875\n",
      "Epoch 2, loss : 1.9958, val_loss : 4.6174, acc : 0.750, val_acc : 0.688\n",
      "Epoch 3, loss : 0.0000, val_loss : 0.0722, acc : 1.000, val_acc : 0.938\n",
      "Epoch 4, loss : 0.0000, val_loss : 1.2121, acc : 1.000, val_acc : 0.812\n",
      "Epoch 5, loss : 1.1274, val_loss : 3.8047, acc : 0.750, val_acc : 0.750\n",
      "Epoch 6, loss : 0.0141, val_loss : 4.0030, acc : 1.000, val_acc : 0.562\n",
      "Epoch 7, loss : 1.7170, val_loss : 6.1695, acc : 0.750, val_acc : 0.750\n",
      "Epoch 8, loss : 0.0115, val_loss : 4.5835, acc : 1.000, val_acc : 0.562\n",
      "Epoch 9, loss : 0.0008, val_loss : 4.3146, acc : 1.000, val_acc : 0.750\n",
      "test_acc : 0.850\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#Getting the Iris dataset and preparing the variables and target\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "# Convert labels to numbers\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Further split into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    # Iterator to get the mini-batch\n",
    "    \n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# Configure hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# Determine the form of the arguments to be passed to the computational graph\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# train's mini-batch iterator\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    A simple three-layer neural network\n",
    "    \"\"\"\n",
    "    # Declaring weights and biases\n",
    "\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    \n",
    "    # tf.add and + are equivalent\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "\n",
    "# Load the network structure \n",
    "logits = example_net(X)\n",
    "\n",
    "# Objective function\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "# Optimization methods\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Estimation results\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "\n",
    "# Index value calculation\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variable\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run a computational graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop per epoch\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # Loop for each mini-batch\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that the Weights initialization is done through tf.Variable(tf.random_normal); Adam is used as an Optimizer and the activation function passes through tf.nn.relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's use all three types of objective variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 59.9530, val_loss : 34.1425, acc : 0.333, val_acc : 0.708\n",
      "Epoch 1, loss : 10.7681, val_loss : 32.4423, acc : 0.667, val_acc : 0.625\n",
      "Epoch 2, loss : 8.2546, val_loss : 13.9162, acc : 0.333, val_acc : 0.708\n",
      "Epoch 3, loss : 4.9253, val_loss : 12.9182, acc : 0.667, val_acc : 0.708\n",
      "Epoch 4, loss : 0.0000, val_loss : 4.4483, acc : 1.000, val_acc : 0.792\n",
      "Epoch 5, loss : 0.0000, val_loss : 3.0701, acc : 1.000, val_acc : 0.750\n",
      "Epoch 6, loss : 0.0000, val_loss : 2.9091, acc : 1.000, val_acc : 0.833\n",
      "Epoch 7, loss : 0.0000, val_loss : 2.2377, acc : 1.000, val_acc : 0.792\n",
      "Epoch 8, loss : 0.0000, val_loss : 2.6913, acc : 1.000, val_acc : 0.833\n",
      "Epoch 9, loss : 0.0000, val_loss : 1.7083, acc : 1.000, val_acc : 0.875\n",
      "test_acc : 0.967\n"
     ]
    }
   ],
   "source": [
    "#Getting the Iris dataset and preparing the variables and target\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "# Convert labels to numbers\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Further split into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# One-hot encoding of correct label value\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train)\n",
    "y_val_one_hot = enc.transform(y_val)\n",
    "y_test_one_hot = enc.transform(y_test)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    # Iterator to get the mini-batch\n",
    "    \n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# Configure hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# Determine the form of the arguments to be passed to the computational graph\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# train's mini-batch iterator\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    A simple three-layer neural network\n",
    "    \"\"\"\n",
    "    # Declaring weights and biases\n",
    "    \n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    " \n",
    "# Load the network structure \n",
    "logits = example_net(X)\n",
    "\n",
    "# Objective function\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "# Optimization methods\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Estimation results\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "\n",
    "# Index value calculation\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variable\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run a computational graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop per epoch\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # Loop for each mini-batch\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a model usiing the House Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 978712.6875, val_loss : 950517.5625\n",
      "Epoch 1, loss : 270935.8438, val_loss : 281188.5938\n",
      "Epoch 2, loss : 93045.5781, val_loss : 92479.0781\n",
      "Epoch 3, loss : 69939.6328, val_loss : 77162.4141\n",
      "Epoch 4, loss : 37860.3750, val_loss : 37960.7461\n",
      "Epoch 5, loss : 34298.5273, val_loss : 33696.7930\n",
      "Epoch 6, loss : 36147.3633, val_loss : 35032.2500\n",
      "Epoch 7, loss : 34051.8242, val_loss : 32357.3926\n",
      "Epoch 8, loss : 25782.6836, val_loss : 22496.0312\n",
      "Epoch 9, loss : 18403.6270, val_loss : 14920.8457\n",
      "test_mse : 18403.627\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Getting the Iris dataset and preparing the variables and target\n",
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "y = np.log(y)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Further split into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    # Iterator to get the mini-batch\n",
    "    \n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# Configure hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# Determine the form of the arguments to be passed to the computational graph\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# train's mini-batch iterator\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    A simple three-layer neural network\n",
    "    \"\"\"\n",
    "    # Declaring weights and biases    \n",
    "    \n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "\n",
    "# Load the network structure \n",
    "logits = example_net(X)\n",
    "\n",
    "# Objective function\n",
    "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "\n",
    "# Optimization methods\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Estimation results\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "y_pred = logits\n",
    "\n",
    "# Initialize the variable\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run a computational graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop per epoch\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # Loop for each mini-batch\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
    "        loss_list.append(loss)\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        val_loss_list.append(val_loss)    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "    print(\"test_mse : {:.3f}\".format(loss))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(loss_list, label='loss')\n",
    "    plt.plot(val_loss_list, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally create a model that classifies the MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 0.5771, val_loss : 0.9575, acc : 0.800, val_acc : 0.691\n",
      "Epoch 1, loss : 0.6700, val_loss : 0.7562, acc : 0.700, val_acc : 0.808\n",
      "Epoch 2, loss : 1.1256, val_loss : 0.4887, acc : 0.700, val_acc : 0.878\n",
      "Epoch 3, loss : 1.1806, val_loss : 0.4416, acc : 0.700, val_acc : 0.904\n",
      "Epoch 4, loss : 0.6412, val_loss : 0.3699, acc : 0.700, val_acc : 0.911\n",
      "Epoch 5, loss : 0.3425, val_loss : 0.3719, acc : 0.900, val_acc : 0.919\n",
      "Epoch 6, loss : 1.0487, val_loss : 0.3761, acc : 0.700, val_acc : 0.924\n",
      "Epoch 7, loss : 0.3304, val_loss : 0.3340, acc : 0.900, val_acc : 0.929\n",
      "Epoch 8, loss : 1.0305, val_loss : 0.3933, acc : 0.700, val_acc : 0.921\n",
      "Epoch 9, loss : 0.2690, val_loss : 0.3698, acc : 1.000, val_acc : 0.926\n",
      "test_acc : 0.929\n"
     ]
    }
   ],
   "source": [
    "# Download the MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Flatten\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# Type conversion, normalization\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
    "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:])\n",
    "\n",
    "#Split into train and validations sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \n",
    "    # Iterator to get the mini-batch\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# Configure hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# Determine the form of the arguments to be passed to the computational graph\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# train's mini-batch iterator\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    A simple three-layer neural network\n",
    "    \"\"\"\n",
    "    # Declaring weights and biases  \n",
    "    \n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "\n",
    "# Load the network structure                                \n",
    "logits = example_net(X)\n",
    "\n",
    "# Objective function\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "# Optimization methods\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Estimation results\n",
    "max_Y = (tf.argmax(Y, 1))\n",
    "max_Y_pred = tf.argmax(logits, 1)\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "#initialize the variable\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run a computational graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Loop per epoch\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # Loop for each mini-batch\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
